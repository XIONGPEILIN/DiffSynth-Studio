import torch, os
from PIL import Image

# Á°Æ‰øùËÉΩ‰ªé examples ÁõÆÂΩïÊ≠£Á°ÆÂØºÂÖ•
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from model_training.train2509 import QwenImageTrainingModule, ControlNetInput

from diffsynth.trainers.utils import qwen_image_parser
from diffsynth.trainers.unified_dataset import UnifiedDataset, ToAbsolutePath, LoadImage


def print_structure(d, indent=0):
    """‰∏Ä‰∏™ËæÖÂä©ÂáΩÊï∞ÔºåÁî®‰∫éÊºÇ‰∫ÆÂú∞ÊâìÂç∞Âá∫ÂµåÂ•óÊï∞ÊçÆÁªìÊûÑÂèäÂÖ∂ÂΩ¢Áä∂/Á±ªÂûã„ÄÇ"""
    for key, value in d.items():
        print('  ' * indent + f"'{str(key)}'", end=': ')
        if isinstance(value, dict):
            print()
            print_structure(value, indent + 1)
        elif isinstance(value, list):
            if len(value) > 0:
                item = value[0]
                if isinstance(item, torch.Tensor):
                    print(f"List[{len(value)}] of Tensors, first shape: {item.shape}, dtype: {item.dtype}")
                elif isinstance(item, Image.Image):
                     print(f"List[{len(value)}] of PIL.Image, first size: {item.size}")
                elif hasattr(item, '__dict__'): # For objects like ControlNetInput
                    print(f"List[{len(value)}] of {type(item).__name__} objects. First item details:")
                    for k, v in item.__dict__.items():
                        if isinstance(v, Image.Image):
                            print('  ' * (indent + 2) + f"- {k}: PIL.Image with size {v.size}")
                        else:
                            print('  ' * (indent + 2) + f"- {k}: {type(v)}")
                else:
                    print(f"List[{len(value)}] of {type(item)}")
            else:
                print("[] (Empty list)")
        elif isinstance(value, torch.Tensor):
            print(f"Tensor with shape: {value.shape}, dtype: {value.dtype}")
        elif isinstance(value, Image.Image):
            print(f"PIL.Image with size: {value.size}")
        else:
            print(f"{type(value).__name__} ({value})")


if __name__ == "__main__":
    # 0. Ê£ÄÊµãGPU
    if torch.cuda.is_available():
        device = "cuda"
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"üöÄ GPU detected: {gpu_name} ({gpu_memory:.1f} GB)")
    else:
        device = "cpu"
        print("‚ö†Ô∏è  No GPU detected, using CPU")
    
    # 1. ‰∏∫Êñπ‰æøË∞ÉËØïÔºåÁõ¥Êé•Âú®Ê≠§Â§ÑÁ°¨ÁºñÁ†ÅÂèÇÊï∞
    from types import SimpleNamespace
    args = SimpleNamespace(
        dataset_base_path="prepared_data_original",
        dataset_metadata_path="prepared_data_original/metadata.json",
        data_file_keys="tgt_original_file,tgt_original_mask_file",
        height=1024,
        width=1024,
        max_pixels=1920*1080, # A reasonable default
        model_id_with_origin_paths="Qwen/Qwen-Image-Edit-2509:transformer/diffusion_pytorch_model*.safetensors,Qwen/Qwen-Image:text_encoder/model*.safetensors,Qwen/Qwen-Image:vae/diffusion_pytorch_model.safetensors,DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint:model.safetensors",
        lora_base_model="dit",
        lora_target_modules="to_q,to_k,to_v,add_q_proj,add_k_proj,add_v_proj,to_out.0,to_add_out,img_mlp.net.2,img_mod.1,txt_mlp.net.2,txt_mod.1",
        lora_rank=128,
        extra_inputs="blockwise_controlnet_image,blockwise_controlnet_inpaint_mask",
        batch_size=4,  # ËÆæÁΩÆbatchÂ§ßÂ∞èÔºå1‰∏∫ÂçïÊ†∑Êú¨Ôºå>1‰∏∫Â§öbatch
        device=device,  # ‰ΩøÁî®Ê£ÄÊµãÂà∞ÁöÑdevice
    )
    print("\n--- 0. Using Hardcoded Arguments for Debugging ---")
    print(f"Device: {args.device}")
    print(f"Batch Size: {args.batch_size}")

    # 2. ÊûÑÂª∫Êï∞ÊçÆÈõÜ (‰∏é train2509.py ÂÆåÂÖ®‰∏ÄËá¥)
    print("\n--- 1. Building Dataset ---")
    dataset = UnifiedDataset(
        base_path=args.dataset_base_path,
        metadata_path=args.dataset_metadata_path,
        repeat=1, # For debugging, we don't need repeats
        data_file_keys=args.data_file_keys.split(","),
        main_data_operator=UnifiedDataset.default_image_operator(
            base_path=args.dataset_base_path,
            height=args.height,
            width=args.width,
            height_division_factor=16,
            width_division_factor=16,
        )
    )
    print(f"Dataset built successfully. Total samples: {len(dataset)}\n")

    # 3. Âä†ËΩΩÊ®°Âûã (‰∏é train2509.py ÂÆåÂÖ®‰∏ÄËá¥)
    print("--- 2. Loading Model ---")
    model = QwenImageTrainingModule(
        model_id_with_origin_paths=args.model_id_with_origin_paths,
        lora_base_model=args.lora_base_model,
        lora_target_modules=args.lora_target_modules,
        lora_rank=args.lora_rank,
        extra_inputs=args.extra_inputs,
        # Provide default values for other potential arguments
        model_paths=None,
        tokenizer_path=None, processor_path=None,
        trainable_models=None, lora_checkpoint=None,
        use_gradient_checkpointing=True, use_gradient_checkpointing_offload=False,
        enable_fp8_training=False, task="sft",
        device=args.device,  # ‰ΩøÁî®Ê£ÄÊµãÂà∞ÁöÑGPU
    )
    print(f"Model loaded successfully on {args.device}.\n")
    
    # ÊòæÁ§∫GPUÂÜÖÂ≠ò‰ΩøÁî®ÊÉÖÂÜµ
    if args.device == "cuda":
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"üìä GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\n")

    # 4. Ëé∑ÂèñÊï∞ÊçÆÊ†∑Êú¨Âπ∂ÊûÑÂª∫batch
    if args.batch_size == 1:
        print("--- 3. Fetching and Preprocessing a Single Sample ---")
        single_data_sample = dataset[0]
        print(f"Sample prompt: '{single_data_sample['prompt'][:80]}...'\n")
        
        # ‰∏∫‰∫ÜËÆ©ËÑöÊú¨ËÉΩÂ§ÑÁêÜÊâπÊ¨°‰∏∫1ÁöÑÊï∞ÊçÆÔºåÊàë‰ª¨Â∞ÜÊØè‰∏™ÂÄºÂåÖË£ÖÂú®ÂàóË°®‰∏≠
        batch_sample = {key: [value] for key, value in single_data_sample.items()}
    else:
        print(f"--- 3. Fetching and Building Batch with {args.batch_size} Samples ---")
        batch_samples = []
        actual_batch_size = min(args.batch_size, len(dataset))
        
        for i in range(actual_batch_size):
            sample = dataset[i]
            batch_samples.append(sample)
            prompt_preview = sample['prompt'][:60] + "..." if len(sample['prompt']) > 60 else sample['prompt']
            print(f"  Sample {i+1}: '{prompt_preview}'")
        
        # ÊûÑÂª∫batchÔºöÂ∞ÜÂ§ö‰∏™Ê†∑Êú¨ÁöÑÊØè‰∏™keyÂêàÂπ∂‰∏∫list
        batch_sample = {}
        for key in batch_samples[0].keys():
            batch_sample[key] = [sample[key] for sample in batch_samples]
        
        print(f"\nBatch constructed with {actual_batch_size} samples.\n")

    # 5. ËøêË°åÈ¢ÑÂ§ÑÁêÜ
    print("--- 4. Running Preprocessing ---")
    try:
        final_inputs = model.forward_preprocess(batch_sample)
        print("Preprocessing complete.\n")
        
        # 6. ÊâìÂç∞ÊúÄÁªàÁöÑÊ®°ÂûãËæìÂÖ•ÁªìÊûÑ
        print("="*50)
        if args.batch_size == 1:
            print("          FINAL MODEL INPUTS STRUCTURE")
        else:
            print(f"     FINAL BATCH MODEL INPUTS (size={args.batch_size})")
        print("="*50)
        print_structure(final_inputs)
        print("="*50)
        
        # 7. È™åËØÅbatchÁª¥Â∫¶
        if args.batch_size > 1:
            print("\n--- Batch Dimension Verification ---")
            for key, value in final_inputs.items():
                if isinstance(value, torch.Tensor):
                    print(f"  {key}: batch_dim={value.shape[0]}, expected={min(args.batch_size, len(dataset))}")
        
        # 8. ÊòæÁ§∫ÊúÄÁªàGPUÂÜÖÂ≠ò‰ΩøÁî®
        if args.device == "cuda":
            print("\n--- Final GPU Memory Usage ---")
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            max_allocated = torch.cuda.max_memory_allocated(0) / 1024**3
            print(f"  Current: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved")
            print(f"  Peak: {max_allocated:.2f} GB allocated")
        
        print("\n‚úÖ Debug script finished. Check the structure above to verify model inputs.")
    except Exception as e:
        print(f"\n‚ùå Error during preprocessing: {e}")
        import traceback
        traceback.print_exc()
