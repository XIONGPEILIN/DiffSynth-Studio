{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aa624be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/student/s2430069/.local/share/mamba/envs/diffy/lib/python3.13/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型中的键:\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.0.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.0.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.0.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.0.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.0.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.0.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.1.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.1.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.1.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.1.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.1.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.1.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.10.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.10.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.10.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.10.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.10.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.10.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.11.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.11.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.11.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.11.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.11.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.11.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.12.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.12.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.12.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.12.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.12.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.12.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.13.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.13.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.13.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.13.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.13.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.13.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.14.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.14.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.14.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.14.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.14.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.14.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.15.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.15.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.15.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.15.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.15.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.15.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.16.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.16.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.16.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.16.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.16.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.16.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.17.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.17.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.17.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.17.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.17.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.17.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.18.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.18.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.18.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.18.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.18.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.18.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.19.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.19.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.19.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.19.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.19.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.19.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.2.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.2.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.2.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.2.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.2.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.2.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.20.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.20.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.20.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.20.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.20.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.20.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.21.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.21.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.21.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.21.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.21.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.21.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.22.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.22.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.22.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.22.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.22.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.22.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.23.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.23.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.23.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.23.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.23.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.23.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.24.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.24.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.24.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.24.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.24.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.24.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.25.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.25.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.25.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.25.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.25.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.25.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.26.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.26.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.26.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.26.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.26.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.26.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.27.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.27.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.27.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.27.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.27.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.27.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.28.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.28.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.28.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.28.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.28.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.28.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.29.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.29.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.29.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.29.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.29.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.29.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.3.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.3.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.3.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.3.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.3.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.3.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.30.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.30.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.30.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.30.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.30.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.30.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.31.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.31.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.31.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.31.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.31.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.31.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.32.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.32.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.32.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.32.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.32.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.32.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.33.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.33.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.33.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.33.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.33.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.33.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.34.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.34.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.34.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.34.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.34.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.34.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.35.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.35.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.35.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.35.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.35.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.35.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.36.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.36.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.36.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.36.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.36.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.36.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.37.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.37.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.37.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.37.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.37.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.37.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.38.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.38.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.38.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.38.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.38.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.38.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.39.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.39.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.39.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.39.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.39.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.39.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.4.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.4.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.4.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.4.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.4.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.4.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.40.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.40.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.40.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.40.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.40.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.40.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.41.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.41.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.41.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.41.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.41.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.41.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.42.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.42.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.42.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.42.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.42.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.42.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.43.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.43.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.43.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.43.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.43.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.43.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.44.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.44.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.44.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.44.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.44.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.44.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.45.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.45.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.45.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.45.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.45.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.45.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.46.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.46.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.46.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.46.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.46.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.46.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.47.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.47.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.47.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.47.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.47.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.47.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.48.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.48.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.48.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.48.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.48.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.48.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.49.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.49.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.49.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.49.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.49.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.49.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.5.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.5.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.5.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.5.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.5.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.5.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.50.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.50.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.50.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.50.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.50.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.50.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.51.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.51.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.51.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.51.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.51.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.51.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.52.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.52.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.52.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.52.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.52.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.52.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.53.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.53.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.53.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.53.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.53.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.53.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.54.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.54.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.54.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.54.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.54.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.54.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.55.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.55.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.55.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.55.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.55.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.55.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.56.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.56.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.56.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.56.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.56.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.56.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.57.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.57.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.57.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.57.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.57.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.57.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.58.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.58.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.58.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.58.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.58.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.58.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.59.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.59.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.59.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.59.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.59.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.59.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.6.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.6.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.6.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.6.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.6.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.6.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.7.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.7.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.7.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.7.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.7.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.7.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.8.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.8.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.8.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.8.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.8.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.8.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.9.input_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.9.input_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.9.output_proj.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.9.output_proj.weight: torch.Size([3072, 3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.9.x_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.controlnet_blocks.9.y_rms.weight: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.img_in.bias: torch.Size([3072])\n",
      "- pipe.blockwise_controlnet.models.0.img_in.weight: torch.Size([3072, 320])\n",
      "- transformer_blocks.0.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.0.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.0.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.0.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.0.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.0.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.0.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.0.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.0.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.0.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.0.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.0.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.0.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.0.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.0.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.0.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.0.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.0.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.0.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.0.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.0.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.0.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.0.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.0.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.1.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.1.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.1.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.1.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.1.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.1.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.1.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.1.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.1.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.1.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.1.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.1.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.1.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.1.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.1.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.1.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.1.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.1.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.1.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.1.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.1.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.1.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.1.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.1.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.10.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.10.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.10.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.10.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.10.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.10.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.10.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.10.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.10.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.10.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.10.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.10.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.10.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.10.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.10.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.10.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.10.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.10.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.10.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.10.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.10.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.10.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.10.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.10.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.11.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.11.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.11.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.11.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.11.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.11.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.11.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.11.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.11.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.11.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.11.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.11.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.11.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.11.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.11.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.11.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.11.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.11.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.11.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.11.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.11.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.11.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.11.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.11.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.12.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.12.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.12.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.12.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.12.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.12.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.12.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.12.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.12.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.12.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.12.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.12.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.12.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.12.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.12.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.12.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.12.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.12.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.12.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.12.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.12.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.12.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.12.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.12.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.13.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.13.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.13.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.13.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.13.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.13.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.13.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.13.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.13.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.13.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.13.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.13.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.13.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.13.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.13.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.13.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.13.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.13.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.13.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.13.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.13.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.13.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.13.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.13.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.14.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.14.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.14.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.14.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.14.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.14.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.14.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.14.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.14.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.14.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.14.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.14.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.14.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.14.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.14.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.14.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.14.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.14.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.14.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.14.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.14.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.14.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.14.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.14.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.15.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.15.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.15.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.15.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.15.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.15.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.15.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.15.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.15.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.15.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.15.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.15.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.15.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.15.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.15.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.15.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.15.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.15.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.15.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.15.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.15.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.15.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.15.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.15.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.16.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.16.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.16.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.16.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.16.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.16.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.16.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.16.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.16.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.16.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.16.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.16.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.16.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.16.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.16.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.16.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.16.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.16.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.16.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.16.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.16.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.16.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.16.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.16.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.17.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.17.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.17.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.17.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.17.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.17.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.17.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.17.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.17.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.17.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.17.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.17.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.17.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.17.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.17.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.17.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.17.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.17.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.17.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.17.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.17.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.17.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.17.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.17.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.18.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.18.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.18.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.18.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.18.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.18.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.18.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.18.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.18.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.18.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.18.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.18.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.18.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.18.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.18.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.18.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.18.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.18.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.18.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.18.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.18.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.18.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.18.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.18.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.19.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.19.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.19.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.19.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.19.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.19.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.19.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.19.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.19.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.19.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.19.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.19.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.19.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.19.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.19.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.19.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.19.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.19.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.19.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.19.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.19.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.19.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.19.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.19.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.2.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.2.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.2.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.2.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.2.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.2.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.2.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.2.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.2.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.2.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.2.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.2.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.2.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.2.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.2.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.2.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.2.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.2.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.2.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.2.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.2.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.2.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.2.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.2.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.20.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.20.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.20.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.20.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.20.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.20.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.20.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.20.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.20.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.20.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.20.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.20.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.20.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.20.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.20.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.20.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.20.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.20.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.20.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.20.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.20.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.20.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.20.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.20.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.21.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.21.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.21.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.21.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.21.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.21.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.21.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.21.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.21.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.21.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.21.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.21.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.21.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.21.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.21.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.21.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.21.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.21.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.21.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.21.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.21.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.21.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.21.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.21.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.22.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.22.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.22.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.22.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.22.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.22.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.22.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.22.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.22.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.22.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.22.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.22.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.22.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.22.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.22.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.22.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.22.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.22.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.22.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.22.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.22.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.22.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.22.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.22.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.23.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.23.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.23.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.23.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.23.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.23.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.23.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.23.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.23.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.23.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.23.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.23.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.23.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.23.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.23.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.23.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.23.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.23.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.23.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.23.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.23.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.23.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.23.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.23.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.24.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.24.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.24.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.24.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.24.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.24.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.24.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.24.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.24.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.24.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.24.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.24.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.24.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.24.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.24.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.24.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.24.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.24.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.24.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.24.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.24.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.24.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.24.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.24.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.25.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.25.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.25.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.25.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.25.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.25.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.25.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.25.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.25.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.25.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.25.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.25.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.25.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.25.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.25.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.25.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.25.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.25.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.25.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.25.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.25.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.25.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.25.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.25.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.26.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.26.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.26.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.26.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.26.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.26.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.26.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.26.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.26.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.26.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.26.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.26.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.26.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.26.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.26.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.26.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.26.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.26.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.26.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.26.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.26.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.26.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.26.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.26.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.27.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.27.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.27.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.27.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.27.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.27.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.27.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.27.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.27.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.27.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.27.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.27.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.27.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.27.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.27.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.27.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.27.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.27.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.27.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.27.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.27.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.27.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.27.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.27.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.28.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.28.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.28.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.28.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.28.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.28.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.28.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.28.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.28.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.28.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.28.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.28.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.28.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.28.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.28.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.28.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.28.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.28.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.28.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.28.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.28.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.28.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.28.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.28.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.29.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.29.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.29.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.29.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.29.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.29.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.29.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.29.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.29.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.29.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.29.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.29.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.29.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.29.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.29.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.29.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.29.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.29.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.29.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.29.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.29.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.29.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.29.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.29.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.3.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.3.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.3.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.3.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.3.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.3.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.3.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.3.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.3.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.3.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.3.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.3.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.3.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.3.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.3.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.3.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.3.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.3.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.3.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.3.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.3.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.3.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.3.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.3.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.30.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.30.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.30.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.30.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.30.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.30.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.30.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.30.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.30.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.30.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.30.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.30.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.30.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.30.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.30.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.30.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.30.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.30.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.30.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.30.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.30.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.30.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.30.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.30.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.31.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.31.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.31.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.31.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.31.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.31.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.31.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.31.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.31.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.31.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.31.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.31.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.31.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.31.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.31.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.31.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.31.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.31.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.31.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.31.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.31.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.31.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.31.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.31.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.32.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.32.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.32.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.32.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.32.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.32.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.32.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.32.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.32.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.32.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.32.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.32.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.32.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.32.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.32.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.32.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.32.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.32.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.32.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.32.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.32.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.32.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.32.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.32.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.33.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.33.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.33.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.33.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.33.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.33.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.33.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.33.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.33.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.33.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.33.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.33.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.33.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.33.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.33.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.33.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.33.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.33.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.33.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.33.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.33.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.33.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.33.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.33.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.34.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.34.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.34.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.34.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.34.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.34.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.34.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.34.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.34.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.34.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.34.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.34.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.34.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.34.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.34.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.34.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.34.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.34.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.34.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.34.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.34.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.34.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.34.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.34.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.35.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.35.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.35.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.35.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.35.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.35.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.35.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.35.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.35.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.35.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.35.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.35.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.35.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.35.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.35.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.35.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.35.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.35.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.35.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.35.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.35.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.35.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.35.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.35.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.36.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.36.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.36.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.36.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.36.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.36.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.36.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.36.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.36.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.36.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.36.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.36.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.36.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.36.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.36.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.36.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.36.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.36.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.36.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.36.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.36.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.36.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.36.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.36.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.37.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.37.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.37.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.37.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.37.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.37.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.37.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.37.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.37.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.37.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.37.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.37.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.37.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.37.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.37.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.37.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.37.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.37.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.37.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.37.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.37.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.37.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.37.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.37.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.38.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.38.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.38.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.38.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.38.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.38.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.38.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.38.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.38.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.38.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.38.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.38.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.38.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.38.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.38.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.38.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.38.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.38.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.38.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.38.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.38.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.38.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.38.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.38.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.39.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.39.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.39.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.39.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.39.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.39.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.39.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.39.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.39.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.39.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.39.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.39.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.39.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.39.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.39.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.39.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.39.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.39.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.39.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.39.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.39.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.39.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.39.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.39.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.4.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.4.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.4.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.4.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.4.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.4.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.4.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.4.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.4.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.4.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.4.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.4.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.4.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.4.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.4.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.4.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.4.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.4.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.4.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.4.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.4.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.4.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.4.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.4.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.40.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.40.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.40.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.40.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.40.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.40.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.40.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.40.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.40.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.40.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.40.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.40.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.40.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.40.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.40.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.40.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.40.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.40.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.40.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.40.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.40.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.40.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.40.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.40.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.41.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.41.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.41.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.41.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.41.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.41.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.41.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.41.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.41.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.41.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.41.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.41.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.41.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.41.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.41.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.41.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.41.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.41.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.41.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.41.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.41.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.41.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.41.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.41.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.42.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.42.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.42.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.42.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.42.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.42.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.42.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.42.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.42.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.42.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.42.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.42.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.42.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.42.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.42.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.42.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.42.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.42.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.42.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.42.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.42.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.42.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.42.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.42.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.43.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.43.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.43.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.43.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.43.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.43.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.43.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.43.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.43.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.43.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.43.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.43.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.43.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.43.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.43.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.43.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.43.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.43.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.43.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.43.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.43.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.43.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.43.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.43.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.44.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.44.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.44.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.44.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.44.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.44.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.44.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.44.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.44.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.44.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.44.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.44.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.44.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.44.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.44.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.44.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.44.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.44.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.44.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.44.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.44.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.44.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.44.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.44.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.45.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.45.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.45.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.45.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.45.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.45.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.45.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.45.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.45.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.45.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.45.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.45.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.45.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.45.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.45.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.45.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.45.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.45.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.45.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.45.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.45.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.45.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.45.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.45.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.46.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.46.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.46.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.46.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.46.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.46.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.46.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.46.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.46.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.46.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.46.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.46.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.46.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.46.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.46.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.46.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.46.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.46.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.46.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.46.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.46.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.46.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.46.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.46.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.47.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.47.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.47.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.47.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.47.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.47.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.47.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.47.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.47.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.47.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.47.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.47.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.47.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.47.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.47.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.47.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.47.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.47.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.47.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.47.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.47.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.47.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.47.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.47.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.48.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.48.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.48.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.48.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.48.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.48.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.48.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.48.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.48.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.48.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.48.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.48.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.48.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.48.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.48.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.48.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.48.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.48.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.48.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.48.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.48.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.48.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.48.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.48.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.49.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.49.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.49.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.49.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.49.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.49.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.49.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.49.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.49.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.49.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.49.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.49.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.49.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.49.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.49.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.49.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.49.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.49.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.49.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.49.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.49.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.49.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.49.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.49.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.5.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.5.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.5.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.5.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.5.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.5.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.5.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.5.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.5.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.5.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.5.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.5.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.5.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.5.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.5.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.5.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.5.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.5.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.5.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.5.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.5.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.5.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.5.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.5.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.50.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.50.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.50.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.50.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.50.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.50.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.50.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.50.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.50.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.50.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.50.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.50.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.50.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.50.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.50.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.50.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.50.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.50.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.50.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.50.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.50.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.50.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.50.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.50.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.51.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.51.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.51.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.51.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.51.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.51.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.51.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.51.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.51.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.51.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.51.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.51.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.51.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.51.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.51.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.51.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.51.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.51.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.51.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.51.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.51.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.51.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.51.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.51.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.52.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.52.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.52.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.52.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.52.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.52.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.52.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.52.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.52.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.52.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.52.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.52.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.52.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.52.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.52.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.52.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.52.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.52.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.52.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.52.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.52.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.52.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.52.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.52.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.53.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.53.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.53.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.53.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.53.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.53.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.53.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.53.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.53.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.53.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.53.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.53.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.53.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.53.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.53.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.53.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.53.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.53.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.53.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.53.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.53.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.53.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.53.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.53.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.54.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.54.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.54.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.54.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.54.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.54.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.54.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.54.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.54.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.54.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.54.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.54.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.54.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.54.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.54.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.54.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.54.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.54.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.54.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.54.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.54.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.54.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.54.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.54.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.55.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.55.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.55.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.55.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.55.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.55.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.55.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.55.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.55.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.55.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.55.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.55.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.55.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.55.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.55.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.55.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.55.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.55.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.55.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.55.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.55.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.55.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.55.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.55.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.56.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.56.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.56.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.56.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.56.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.56.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.56.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.56.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.56.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.56.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.56.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.56.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.56.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.56.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.56.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.56.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.56.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.56.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.56.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.56.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.56.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.56.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.56.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.56.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.57.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.57.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.57.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.57.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.57.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.57.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.57.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.57.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.57.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.57.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.57.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.57.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.57.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.57.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.57.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.57.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.57.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.57.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.57.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.57.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.57.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.57.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.57.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.57.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.58.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.58.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.58.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.58.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.58.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.58.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.58.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.58.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.58.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.58.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.58.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.58.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.58.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.58.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.58.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.58.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.58.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.58.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.58.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.58.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.58.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.58.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.58.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.58.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.59.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.59.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.59.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.59.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.59.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.59.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.59.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.59.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.59.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.59.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.59.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.59.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.59.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.59.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.59.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.59.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.59.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.59.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.59.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.59.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.59.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.59.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.59.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.59.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.6.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.6.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.6.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.6.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.6.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.6.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.6.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.6.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.6.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.6.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.6.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.6.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.6.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.6.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.6.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.6.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.6.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.6.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.6.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.6.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.6.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.6.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.6.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.6.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.7.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.7.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.7.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.7.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.7.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.7.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.7.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.7.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.7.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.7.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.7.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.7.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.7.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.7.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.7.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.7.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.7.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.7.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.7.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.7.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.7.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.7.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.7.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.7.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.8.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.8.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.8.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.8.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.8.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.8.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.8.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.8.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.8.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.8.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.8.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.8.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.8.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.8.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.8.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.8.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.8.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.8.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.8.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.8.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.8.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.8.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.8.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.8.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.9.attn.add_k_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.9.attn.add_k_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.9.attn.add_q_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.9.attn.add_q_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.9.attn.add_v_proj.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.9.attn.add_v_proj.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.9.attn.to_add_out.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.9.attn.to_add_out.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.9.attn.to_k.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.9.attn.to_k.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.9.attn.to_out.0.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.9.attn.to_out.0.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.9.attn.to_q.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.9.attn.to_q.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.9.attn.to_v.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.9.attn.to_v.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.9.img_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.9.img_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.9.img_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.9.img_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n",
      "- transformer_blocks.9.txt_mlp.net.2.lora_A.default.weight: torch.Size([128, 12288])\n",
      "- transformer_blocks.9.txt_mlp.net.2.lora_B.default.weight: torch.Size([3072, 128])\n",
      "- transformer_blocks.9.txt_mod.1.lora_A.default.weight: torch.Size([128, 3072])\n",
      "- transformer_blocks.9.txt_mod.1.lora_B.default.weight: torch.Size([18432, 128])\n"
     ]
    }
   ],
   "source": [
    "#models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint/model.safetensors\n",
    "\n",
    "import torch\n",
    "from safetensors import safe_open\n",
    "\n",
    "# 使用 safetensors 库来加载 .safetensors 文件\n",
    "with safe_open(\"train/Qwen-Image-Edit-2509_inpaint_controlnet_and_lora/step-1.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "    model = {}\n",
    "    for k in f.keys():\n",
    "        model[k] = f.get_tensor(k)\n",
    "\n",
    "print(\"模型中的键:\")\n",
    "for key in model.keys():\n",
    "    print(f\"- {key}: {model[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16019617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 img_in.weight 形状: torch.Size([3072, 324])\n",
      "新的 img_in.weight 形状: torch.Size([3072, 320])\n",
      "\n",
      "修改后的模型中的键:\n",
      "- controlnet_blocks.0.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.0.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.0.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.0.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.0.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.0.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.1.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.1.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.1.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.1.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.1.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.1.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.10.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.10.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.10.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.10.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.10.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.10.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.11.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.11.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.11.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.11.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.11.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.11.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.12.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.12.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.12.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.12.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.12.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.12.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.13.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.13.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.13.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.13.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.13.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.13.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.14.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.14.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.14.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.14.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.14.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.14.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.15.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.15.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.15.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.15.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.15.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.15.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.16.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.16.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.16.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.16.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.16.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.16.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.17.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.17.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.17.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.17.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.17.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.17.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.18.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.18.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.18.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.18.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.18.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.18.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.19.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.19.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.19.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.19.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.19.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.19.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.2.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.2.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.2.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.2.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.2.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.2.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.20.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.20.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.20.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.20.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.20.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.20.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.21.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.21.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.21.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.21.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.21.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.21.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.22.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.22.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.22.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.22.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.22.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.22.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.23.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.23.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.23.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.23.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.23.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.23.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.24.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.24.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.24.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.24.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.24.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.24.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.25.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.25.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.25.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.25.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.25.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.25.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.26.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.26.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.26.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.26.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.26.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.26.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.27.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.27.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.27.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.27.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.27.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.27.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.28.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.28.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.28.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.28.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.28.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.28.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.29.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.29.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.29.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.29.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.29.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.29.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.3.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.3.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.3.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.3.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.3.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.3.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.30.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.30.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.30.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.30.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.30.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.30.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.31.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.31.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.31.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.31.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.31.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.31.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.32.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.32.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.32.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.32.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.32.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.32.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.33.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.33.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.33.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.33.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.33.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.33.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.34.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.34.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.34.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.34.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.34.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.34.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.35.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.35.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.35.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.35.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.35.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.35.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.36.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.36.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.36.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.36.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.36.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.36.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.37.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.37.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.37.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.37.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.37.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.37.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.38.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.38.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.38.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.38.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.38.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.38.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.39.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.39.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.39.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.39.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.39.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.39.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.4.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.4.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.4.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.4.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.4.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.4.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.40.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.40.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.40.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.40.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.40.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.40.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.41.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.41.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.41.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.41.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.41.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.41.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.42.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.42.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.42.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.42.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.42.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.42.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.43.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.43.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.43.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.43.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.43.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.43.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.44.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.44.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.44.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.44.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.44.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.44.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.45.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.45.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.45.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.45.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.45.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.45.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.46.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.46.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.46.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.46.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.46.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.46.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.47.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.47.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.47.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.47.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.47.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.47.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.48.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.48.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.48.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.48.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.48.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.48.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.49.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.49.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.49.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.49.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.49.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.49.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.5.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.5.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.5.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.5.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.5.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.5.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.50.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.50.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.50.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.50.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.50.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.50.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.51.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.51.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.51.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.51.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.51.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.51.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.52.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.52.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.52.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.52.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.52.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.52.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.53.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.53.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.53.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.53.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.53.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.53.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.54.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.54.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.54.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.54.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.54.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.54.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.55.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.55.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.55.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.55.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.55.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.55.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.56.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.56.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.56.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.56.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.56.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.56.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.57.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.57.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.57.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.57.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.57.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.57.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.58.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.58.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.58.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.58.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.58.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.58.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.59.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.59.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.59.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.59.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.59.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.59.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.6.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.6.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.6.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.6.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.6.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.6.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.7.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.7.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.7.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.7.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.7.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.7.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.8.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.8.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.8.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.8.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.8.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.8.y_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.9.input_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.9.input_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.9.output_proj.bias: torch.Size([3072])\n",
      "- controlnet_blocks.9.output_proj.weight: torch.Size([3072, 3072])\n",
      "- controlnet_blocks.9.x_rms.weight: torch.Size([3072])\n",
      "- controlnet_blocks.9.y_rms.weight: torch.Size([3072])\n",
      "- img_in.bias: torch.Size([3072])\n",
      "- img_in.weight: torch.Size([3072, 320])\n"
     ]
    }
   ],
   "source": [
    "# 修改 img_in.weight 的形状\n",
    "if \"img_in.weight\" in model:\n",
    "    print(f\"原始 img_in.weight 形状: {model['img_in.weight'].shape}\")\n",
    "    \n",
    "    # 删除原来的权重\n",
    "    del model[\"img_in.weight\"]\n",
    "    \n",
    "    # 创建新的权重张量，形状为 [3072, 324]\n",
    "    new_weight = torch.randn(3072, 320, dtype=torch.float32)\n",
    "    model[\"img_in.weight\"] = new_weight\n",
    "    \n",
    "    print(f\"新的 img_in.weight 形状: {model['img_in.weight'].shape}\")\n",
    "else:\n",
    "    print(\"未找到 img_in.weight\")\n",
    "\n",
    "# 显示修改后的模型键信息\n",
    "print(\"\\n修改后的模型中的键:\")\n",
    "for key in model.keys():\n",
    "    print(f\"- {key}: {model[key].shape}\")\n",
    "#保存为models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint/model.safetensors\n",
    "from safetensors.torch import save_file\n",
    "save_file(model, \"models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint/model.safetensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56999885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片路径已批量修正为 ref/ 和 tgt_original/ 子目录。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_path = \"prepared_data_original/metadata.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for item in data:\n",
    "    if item.get(\"ref_file\"):\n",
    "        item[\"ref_file\"] = f\"ref/{item['ref_file']}\"\n",
    "    if item.get(\"tgt_original_file\"):\n",
    "        item[\"tgt_original_file\"] = f\"tgt_original/{item['tgt_original_file']}\"\n",
    "    if item.get(\"tgt_original_mask_file\"):\n",
    "        item[\"tgt_original_mask_file\"] = f\"tgt_original/{item['tgt_original_mask_file']}\"\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"图片路径已批量修正为 ref/ 和 tgt_original/ 子目录。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37a6a269",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m item.get(\u001b[33m\"\u001b[39m\u001b[33mref_file\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     16\u001b[39m     ref_path = os.path.join(base_ref, item[\u001b[33m\"\u001b[39m\u001b[33mref_file\u001b[39m\u001b[33m\"\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     18\u001b[39m         valid_ref += \u001b[32m1\u001b[39m\n\u001b[32m     19\u001b[39m         item[\u001b[33m\"\u001b[39m\u001b[33mref_file\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mref/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[33m'\u001b[39m\u001b[33mref_file\u001b[39m\u001b[33m'\u001b[39m].split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen genericpath>:39\u001b[39m, in \u001b[36misfile\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "json_path = \"prepared_data_original/metadata.json\"\n",
    "base_ref = \"prepared_data_original/ref\"\n",
    "base_tgt = \"prepared_data_original/tgt_original\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 统计有效图片数量\n",
    "valid_ref = 0\n",
    "valid_tgt = 0\n",
    "for item in data:\n",
    "    if item.get(\"ref_file\"):\n",
    "        ref_path = os.path.join(base_ref, item[\"ref_file\"].split(\"/\")[-1])\n",
    "        if os.path.isfile(ref_path):\n",
    "            valid_ref += 1\n",
    "            item[\"ref_file\"] = f\"ref/{item['ref_file'].split('/')[-1]}\"\n",
    "        else:\n",
    "            item[\"ref_file\"] = None\n",
    "    if item.get(\"tgt_original_file\"):\n",
    "        tgt_path = os.path.join(base_tgt, item[\"tgt_original_file\"].split(\"/\")[-1])\n",
    "        if os.path.isfile(tgt_path):\n",
    "            valid_tgt += 1\n",
    "            item[\"tgt_original_file\"] = f\"tgt_original/{item['tgt_original_file'].split('/')[-1]}\"\n",
    "        else:\n",
    "            item[\"tgt_original_file\"] = None\n",
    "    if item.get(\"tgt_original_mask_file\"):\n",
    "        mask_path = os.path.join(base_tgt, item[\"tgt_original_mask_file\"].split(\"/\")[-1])\n",
    "        if os.path.isfile(mask_path):\n",
    "            item[\"tgt_original_mask_file\"] = f\"tgt_original/{item['tgt_original_mask_file'].split('/')[-1]}\"\n",
    "        else:\n",
    "            item[\"tgt_original_mask_file\"] = None\n",
    "\n",
    "# 删除无效图片的条目\n",
    "data = [item for item in data if item.get(\"ref_file\") and item.get(\"tgt_original_file\")]\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"有效ref图片数量: {valid_ref}\")\n",
    "print(f\"有效tgt_original图片数量: {valid_tgt}\")\n",
    "print(\"无效图片条目已删除，路径已修正。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26aaa7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "检查图片: 100%|██████████| 111767/111767 [01:11<00:00, 1570.22it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有效ref图片数量: 111767\n",
      "有效tgt_original图片数量: 111767\n",
      "无效图片条目已删除，路径已修正。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "json_path = \"prepared_data_original/metadata.json\"\n",
    "base_ref = \"prepared_data_original/ref\"\n",
    "base_tgt = \"prepared_data_original/tgt_original\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def check_and_fix(item):\n",
    "    result = item.copy()\n",
    "    valid_ref = False\n",
    "    valid_tgt = False\n",
    "    # 检查ref图片\n",
    "    if item.get(\"ref_file\"):\n",
    "        ref_name = item[\"ref_file\"].split(\"/\")[-1]\n",
    "        ref_path = os.path.join(base_ref, ref_name)\n",
    "        if os.path.isfile(ref_path):\n",
    "            result[\"ref_file\"] = f\"ref/{ref_name}\"\n",
    "            valid_ref = True\n",
    "        else:\n",
    "            result[\"ref_file\"] = None\n",
    "    # 检查tgt图片\n",
    "    if item.get(\"tgt_original_file\"):\n",
    "        tgt_name = item[\"tgt_original_file\"].split(\"/\")[-1]\n",
    "        tgt_path = os.path.join(base_tgt, tgt_name)\n",
    "        if os.path.isfile(tgt_path):\n",
    "            result[\"tgt_original_file\"] = f\"tgt_original/{tgt_name}\"\n",
    "            valid_tgt = True\n",
    "        else:\n",
    "            result[\"tgt_original_file\"] = None\n",
    "    # 检查mask图片\n",
    "    if item.get(\"tgt_original_mask_file\"):\n",
    "        mask_name = item[\"tgt_original_mask_file\"].split(\"/\")[-1]\n",
    "        mask_path = os.path.join(base_tgt, mask_name)\n",
    "        if os.path.isfile(mask_path):\n",
    "            result[\"tgt_original_mask_file\"] = f\"tgt_original/{mask_name}\"\n",
    "        else:\n",
    "            result[\"tgt_original_mask_file\"] = None\n",
    "    return result, valid_ref, valid_tgt\n",
    "\n",
    "new_data = []\n",
    "valid_ref = 0\n",
    "valid_tgt = 0\n",
    "with ThreadPoolExecutor(max_workers=64) as executor:\n",
    "    futures = [executor.submit(check_and_fix, item) for item in data]\n",
    "    for fut in tqdm(as_completed(futures), total=len(futures), desc=\"检查图片\"):\n",
    "        item, ref_ok, tgt_ok = fut.result()\n",
    "        if item.get(\"ref_file\") and item.get(\"tgt_original_file\"):\n",
    "            new_data.append(item)\n",
    "        if ref_ok: valid_ref += 1\n",
    "        if tgt_ok: valid_tgt += 1\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"有效ref图片数量: {valid_ref}\")\n",
    "print(f\"有效tgt_original图片数量: {valid_tgt}\")\n",
    "print(\"无效图片条目已删除，路径已修正。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae37a98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111767"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
